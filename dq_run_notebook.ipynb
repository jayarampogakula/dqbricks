{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "381dc837-226b-46cc-afe1-11f7e4851227",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import yaml\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Resolve repo_root in both normal Python scripts and Databricks notebooks\n",
    "# ---------------------------------------------------------------------\n",
    "try:\n",
    "    # Normal Python script\n",
    "    repo_root = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    # Databricks notebook (no __file__)\n",
    "    repo_root = os.getcwd()\n",
    "\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.append(repo_root)\n",
    "\n",
    "print(f\"[DQBricks] Repo root set to: {repo_root}\")\n",
    "print(f\"[DQBricks] CWD: {os.getcwd()}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Arg handling: strip IPython/Databricks noise like \"-f <json>\"\n",
    "# We'll keep only the first non-flag positional argument, if any.\n",
    "# ---------------------------------------------------------------------\n",
    "def extract_user_cfg_arg(argv: list[str]) -> str | None:\n",
    "    \"\"\"\n",
    "    Returns the first user-supplied positional argument that doesn't look like a flag.\n",
    "    Skips known IPython patterns like '-f <file>'.\n",
    "    \"\"\"\n",
    "    cleaned = []\n",
    "    i = 1  # skip argv[0]\n",
    "    while i < len(argv):\n",
    "        a = argv[i]\n",
    "        # Handle the common \"-f <connection.json>\" pair injected by IPython kernels\n",
    "        if a == \"-f\" and (i + 1) < len(argv):\n",
    "            i += 2\n",
    "            continue\n",
    "        # Skip other flags (e.g., --ip, --something)\n",
    "        if a.startswith(\"-\"):\n",
    "            i += 1\n",
    "            continue\n",
    "        # Take positional args\n",
    "        cleaned.append(a)\n",
    "        i += 1\n",
    "    return cleaned[0] if cleaned else None\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Config path resolution\n",
    "# Defaults to <repo_root>/config/config.yaml\n",
    "# Supports: argv override, relative paths (resolved from repo_root),\n",
    "# absolute workspace paths, and dbfs:/ paths (auto-mapped to /dbfs/...).\n",
    "# ---------------------------------------------------------------------\n",
    "def resolve_cfg_path(argv1: str | None, default_dir: str) -> tuple[str, list[str]]:\n",
    "    tried = []\n",
    "    default_cfg = os.path.join(default_dir, \"config\", \"config.yaml\")\n",
    "\n",
    "    if argv1:\n",
    "        candidates = []\n",
    "        # Map dbfs:/... -> /dbfs/...\n",
    "        if argv1.startswith(\"dbfs:/\"):\n",
    "            candidates.append(\"/dbfs/\" + argv1[len(\"dbfs:/\"):].lstrip(\"/\"))\n",
    "\n",
    "        # Absolute path as-is\n",
    "        if os.path.isabs(argv1):\n",
    "            candidates.append(argv1)\n",
    "        else:\n",
    "            # Relative to repo_root so \"config/config.yaml\" works\n",
    "            candidates.append(os.path.join(default_dir, argv1))\n",
    "\n",
    "        # Also try relative to CWD for completeness\n",
    "        candidates.append(os.path.abspath(argv1))\n",
    "    else:\n",
    "        candidates = [default_cfg]\n",
    "\n",
    "    for p in candidates:\n",
    "        ap = os.path.abspath(p)\n",
    "        tried.append(ap)\n",
    "        if os.path.exists(ap):\n",
    "            return ap, tried\n",
    "\n",
    "    return \"\", tried\n",
    "\n",
    "user_arg = extract_user_cfg_arg(sys.argv)\n",
    "cfg_path, tried_paths = resolve_cfg_path(user_arg, repo_root)\n",
    "\n",
    "if not cfg_path:\n",
    "    msg = [\n",
    "        \"[DQBricks] ERROR: Could not find config.yaml.\",\n",
    "        \"Tried:\",\n",
    "        *[f\"  - {p}\" for p in tried_paths],\n",
    "        \"\",\n",
    "        \"Tips:\",\n",
    "        \"• Keep your config at: <repo_root>/config/config.yaml  (default)\",\n",
    "        \"• Or pass a relative path from repo_root, e.g.:\",\n",
    "        \"    %run /Workspace/.../dqbricks/run_notebook -- config/config.yaml\",\n",
    "        \"• Or pass an absolute Workspace path:\",\n",
    "        \"    %run /Workspace/.../dqbricks/run_notebook -- /Workspace/.../dqbricks/config/config.yaml\",\n",
    "        \"• Or pass a DBFS path (auto-mapped):\",\n",
    "        \"    %run /Workspace/.../dqbricks/run_notebook -- dbfs:/FileStore/dqbricks/config.yaml\",\n",
    "    ]\n",
    "    raise FileNotFoundError(\"\\n\".join(msg))\n",
    "\n",
    "print(f\"[DQBricks] Using config: {cfg_path}\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Spark session\n",
    "# ---------------------------------------------------------------------\n",
    "spark = SparkSession.getActiveSession() or SparkSession.builder.getOrCreate()\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Load config\n",
    "# ---------------------------------------------------------------------\n",
    "with open(cfg_path, \"r\") as f:\n",
    "    cfg = yaml.safe_load(f) or {}\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Imports that rely on repo path being on sys.path\n",
    "# ---------------------------------------------------------------------\n",
    "from dqcore.bootstrap import ensure_objects\n",
    "from dqcore.engine import DQEngine\n",
    "from dqcore.targets import list_tables\n",
    "import rules  # registers built-in rules\n",
    "\n",
    "# Optional custom rules\n",
    "if cfg.get(\"custom_rules_module\"):\n",
    "    importlib.import_module(cfg[\"custom_rules_module\"])\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Ensure quarantine/metrics tables + dashboard views exist\n",
    "# ---------------------------------------------------------------------\n",
    "quarantine_tbl = cfg.get(\"quarantine_table\")\n",
    "metrics_tbl = cfg.get(\"metrics_table\")\n",
    "views_cfg = cfg.get(\"views\", {})\n",
    "\n",
    "if not quarantine_tbl or not metrics_tbl:\n",
    "    raise ValueError(\n",
    "        \"[DQBricks] 'quarantine_table' and 'metrics_table' must be set in config.\"\n",
    "    )\n",
    "\n",
    "ensure_objects(spark, quarantine_tbl, metrics_tbl, views_cfg)\n",
    "\n",
    "engine = DQEngine(spark, quarantine_tbl, metrics_tbl, views_cfg)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Resolve target tables\n",
    "# scope: 'table' | 'schema' | (anything else => by include/exclude)\n",
    "# ---------------------------------------------------------------------\n",
    "scope = cfg.get(\"scope\", \"table\")\n",
    "include = cfg.get(\"include\", [\"*\"])\n",
    "exclude = cfg.get(\"exclude\", [])\n",
    "\n",
    "catalog = cfg.get(\"catalog\")\n",
    "schema = cfg.get(\"schema\")\n",
    "table = cfg.get(\"table\")\n",
    "\n",
    "def fq_tuple(cat: str | None, sch: str, tbl: str) -> tuple[str | None, str, str]:\n",
    "    return (cat, sch, tbl)\n",
    "\n",
    "if scope == \"table\":\n",
    "    if not schema or not table:\n",
    "        raise ValueError(\"[DQBricks] For scope='table', 'schema' and 'table' must be set.\")\n",
    "    targets = [fq_tuple(catalog, schema, table)]\n",
    "elif scope == \"schema\":\n",
    "    if not schema:\n",
    "        raise ValueError(\"[DQBricks] For scope='schema', 'schema' must be set.\")\n",
    "    targets = list_tables(\n",
    "        spark, catalog=catalog, schema=schema, include=include, exclude=exclude\n",
    "    )\n",
    "else:\n",
    "    # catalog or global scope (by include/exclude)\n",
    "    targets = list_tables(\n",
    "        spark, catalog=catalog, include=include, exclude=exclude\n",
    "    )\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Execution mode\n",
    "# ---------------------------------------------------------------------\n",
    "mode = cfg.get(\"mode\", \"batch\").lower()\n",
    "\n",
    "if mode == \"streaming\":\n",
    "    # Auto Loader streaming source\n",
    "    streaming_cfg = cfg.get(\"streaming\", {})\n",
    "    required = [\"input_format\", \"schema_location\", \"input_path\", \"output_table\", \"checkpoint\", \"trigger\"]\n",
    "    missing = [k for k in required if k not in streaming_cfg]\n",
    "    if missing:\n",
    "        raise ValueError(f\"[DQBricks] Missing streaming config keys: {missing}\")\n",
    "\n",
    "    src = (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", streaming_cfg[\"input_format\"])\n",
    "        .option(\"cloudFiles.schemaLocation\", streaming_cfg[\"schema_location\"])\n",
    "        .option(\"rescuedDataColumn\", \"_rescued_data\")\n",
    "        .load(streaming_cfg[\"input_path\"])\n",
    "    )\n",
    "\n",
    "    q = (\n",
    "        engine.enforce_stream(\n",
    "            src,\n",
    "            fq_out_table=streaming_cfg[\"output_table\"],\n",
    "            rule_cfgs=cfg[\"rules\"],\n",
    "            checkpoint=streaming_cfg[\"checkpoint\"],\n",
    "        )\n",
    "        .trigger(streaming_cfg[\"trigger\"])\n",
    "        .start()\n",
    "    )\n",
    "    q.awaitTermination()\n",
    "\n",
    "else:\n",
    "    # -----------------------------\n",
    "    # Batch mode\n",
    "    # -----------------------------\n",
    "    if \"rules\" not in cfg:\n",
    "        raise ValueError(\"[DQBricks] 'rules' must be defined in config for batch mode.\")\n",
    "\n",
    "    inc = cfg.get(\"incremental\", {})\n",
    "\n",
    "    for (cat, sch, tbl) in targets:\n",
    "        fq_table = f\"{cat}.{sch}.{tbl}\" if cat else f\"{sch}.{tbl}\"\n",
    "        print(f\"[DQBricks] Processing table: {fq_table}\")\n",
    "\n",
    "        # Choose rules: per-table override or defaults\n",
    "        rules_cfg = cfg.get(\"tables\", {}).get(fq_table, {}).get(\"rules\", cfg[\"rules\"])\n",
    "        if not rules_cfg:\n",
    "            print(f\"[DQBricks] WARNING: No rules configured for {fq_table}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Read base DataFrame\n",
    "        try:\n",
    "            df = spark.table(fq_table)\n",
    "        except Exception as e:\n",
    "            print(f\"[DQBricks] ERROR: Could not read table {fq_table}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Optional incremental via Delta Change Data Feed\n",
    "        if inc.get(\"use_cdf\", False):\n",
    "            try:\n",
    "                spark.sql(\n",
    "                    f\"ALTER TABLE {fq_table} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\"\n",
    "                )\n",
    "                df = (\n",
    "                    spark.read.format(\"delta\")\n",
    "                    .option(\"readChangeFeed\", \"true\")\n",
    "                    .option(\"startingVersion\", inc.get(\"starting_version\", 0))\n",
    "                    .table(fq_table)\n",
    "                    .filter(\"_change_type IN ('insert','update_postimage')\")\n",
    "                )\n",
    "                print(f\"[DQBricks] CDF enabled for {fq_table} (starting_version={inc.get('starting_version', 0)}).\")\n",
    "            except Exception as e:\n",
    "                print(f\"[DQBricks] WARNING: Failed to use CDF for {fq_table}: {e}. Falling back to full read.\")\n",
    "\n",
    "        try:\n",
    "            _ = engine.enforce_batch(df, fq_table, rules_cfg)\n",
    "            print(f\"[DQBricks] Processed {fq_table} with {len(rules_cfg)} rules.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[DQBricks] ERROR: Enforcement failed for {fq_table}: {e}\")\n",
    "\n",
    "print(\"[DQBricks] run complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a286bfd2-3378-4a9f-bcd6-9cd8debf718a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from dqcore.registry import RuleRegistry\n",
    "print(\"Registered rules:\", sorted(RuleRegistry.names()))\n",
    "# Expect something like:\n",
    "# ['check','freshness','in_set','not_null','range','uniqueness', ... plus your custom 'max_length' if any]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37e5737c-c1c2-4d8e-9623-806fa963538d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS explore.dq_schema;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS explore.dq_schema.sample_data_dq\n",
    "(\n",
    "  id DOUBLE,\n",
    "  email STRING,\n",
    "  amount DOUBLE,\n",
    "  _rescued_data STRING,\n",
    "  nn_id BOOLEAN,\n",
    "  amt_non_negative BOOLEAN,\n",
    "  email_ok BOOLEAN,\n",
    "  _dq_checked_at TIMESTAMP,\n",
    "  _dq_is_valid BOOLEAN,\n",
    "  table_name STRING\n",
    ") USING DELTA;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d429e3d-e709-4e1a-b37b-d5af7fadda46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_out = (\n",
    "    df\n",
    "    .withColumnRenamed(\"nn_id\", \"dq_nn_id\")\n",
    "    .withColumnRenamed(\"amt_non_negative\", \"dq_amt_non_negative\")\n",
    "    .withColumnRenamed(\"email_ok\", \"dq_email_ok\")\n",
    ")\n",
    "\n",
    "(df_out\n",
    " .write\n",
    " .mode(\"append\")\n",
    " .saveAsTable(\"explore.dq_schema.sample_data_dq\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eab96d26-e6eb-4767-857d-d9172b1e3c93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "explore.dq_schema.ncr_ride_bookings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "722e5ea8-2dd8-4de0-ba15-6da9ed5a0ff6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys, importlib\n",
    "\n",
    "# 0) Make sure your project root is first on sys.path\n",
    "sys.path.insert(0, \"/Workspace/Users/jayarampogakula@gmail.com/dqbricks\")\n",
    "\n",
    "# 1) Nuke any stale modules from memory\n",
    "for m in [\"rules\", \"rules.builtin\", \"custom_rules\", \"dqcore.registry\"]:\n",
    "    if m in sys.modules:\n",
    "        sys.modules.pop(m)\n",
    "\n",
    "# 2) Import the SINGLE registry module we'll all use\n",
    "import dqcore.registry as registry\n",
    "print(\"registry @\", registry.__file__)\n",
    "\n",
    "# 3) Import custom rules FIRST (if present). If they try to overwrite, we'll override next.\n",
    "try:\n",
    "    cr = importlib.import_module(\"custom_rules\")\n",
    "    print(\"custom_rules @\", cr.__file__)\n",
    "except ModuleNotFoundError:\n",
    "    cr = None\n",
    "    print(\"custom_rules not found (that's fine)\")\n",
    "\n",
    "# 4) Import (or reload) built-ins LAST so they are definitely registered\n",
    "rb = importlib.import_module(\"rules.builtin\")\n",
    "importlib.reload(rb)\n",
    "print(\"rules.builtin @\", rb.__file__)\n",
    "\n",
    "# 5) Check what’s registered NOW\n",
    "from dqcore.registry import RuleRegistry\n",
    "print(\"Registered rules:\", sorted(RuleRegistry.names()))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4522250065760764,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "dq_run_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
